\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{float}

\title{Eigenvalue Calculation: Algorithm Selection, Analysis, and Implementation}
\author{Charitha}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report discusses the implementation of various algorithms for calculating the eigenvalues of a matrix. After exploring multiple approaches, the QR algorithm was chosen due to its robustness and efficiency. The report provides an analysis of the selected algorithm, time complexity, and the reasoning behind its choice. Additionally, a comparison with other eigenvalue computation methods is presented, including insights into the algorithm's suitability for different types of matrices.
\end{abstract}

\section{Introduction}
Eigenvalue calculation is a fundamental problem in linear algebra, with applications across various fields such as quantum mechanics, vibration analysis, and machine learning. The objective of this assignment is to explore and implement algorithms for computing the eigenvalues of a given matrix, analyze the time complexity of the chosen algorithm, and compare it with other available methods.

\section{Research and Algorithm Selection}

\subsection{Overview of Eigenvalue Calculation Algorithms}
There are several well-known algorithms for eigenvalue calculation. Some of the most commonly used algorithms include:

\begin{itemize}
    \item \textbf{Power Iteration}: A simple iterative method for finding the dominant eigenvalue.
    \item \textbf{Jacobi Method}: Suitable for symmetric matrices, this method diagonalizes the matrix using successive rotations.
    \item \textbf{QR Algorithm}: An iterative method based on QR decomposition, highly suitable for general matrices, including non-symmetric ones.
    \item \textbf{Lanczos Algorithm}: A method that is efficient for sparse matrices.
\end{itemize}

\subsection{Chosen Algorithm: QR Algorithm}
The QR algorithm was chosen for this assignment due to its general applicability and efficiency in computing the eigenvalues of both symmetric and non-symmetric matrices. The QR algorithm operates by iterating through the following steps:

\begin{itemize}
    \item Decompose the matrix \( A \) into a product of an orthogonal matrix \( Q \) and an upper triangular matrix \( R \), i.e., \( A = QR \).
    \item Form a new matrix \( A' = RQ \) and repeat the decomposition process.
    \item The eigenvalues of \( A \) are approximated by the diagonal elements of \( A' \) after several iterations.
\end{itemize}

\subsection{Justification for Algorithm Choice}
The QR algorithm is chosen over other methods due to its efficiency and versatility in handling large and complex matrices. Unlike power iteration, which only finds the dominant eigenvalue, the QR algorithm is capable of computing all eigenvalues, making it more suitable for this assignment.

\section{Implementation and Time Complexity Analysis}

\subsection{Code Implementation}
The program reads a matrix, applies the QR algorithm, and iterates until convergence to compute the eigenvalues. The convergence criteria are met when the off-diagonal elements of the matrix are sufficiently close to zero.

\begin{verbatim}
  // Pseudocode for QR Algorithm:
  matrix A = input_matrix;
  repeat:
    QR_decomposition(A);
    A = R * Q;
  until convergence;
  return diagonal elements of A as eigenvalues;
\end{verbatim}

\subsection{Time Complexity}
The time complexity of the QR algorithm depends on the matrix size \( n \). Each iteration of the algorithm involves performing a QR decomposition, which has a time complexity of \( O(n^3) \). Since the algorithm typically converges in \( O(n) \) iterations, the overall time complexity of the QR algorithm is \( O(n^4) \).

\subsection{Memory Usage}
The QR algorithm requires \( O(n^2) \) memory for storing the matrix and its decompositions. This memory usage is relatively efficient compared to other methods like Jacobi, which might require additional memory for storing rotation matrices.

\section{Comparison of Algorithms}

\subsection{Power Iteration}
\begin{itemize}
    \item \textbf{Time Complexity}: \( O(n^2) \) for each iteration, but only converges for the dominant eigenvalue.
    \item \textbf{Suitability}: Best for finding the largest eigenvalue of a matrix.
\end{itemize}

\subsection{Jacobi Method}
\begin{itemize}
    \item \textbf{Time Complexity}: \( O(n^3) \) per iteration, suitable for symmetric matrices.
    \item \textbf{Suitability}: Efficient for symmetric matrices but less efficient for non-symmetric ones.
\end{itemize}

\subsection{QR Algorithm (Chosen Method)}
\begin{itemize}
    \item \textbf{Time Complexity}: \( O(n^4) \), capable of computing all eigenvalues.
    \item \textbf{Suitability}: Efficient for both symmetric and non-symmetric matrices, suitable for general-purpose eigenvalue calculation.
\end{itemize}

\subsection{Lanczos Algorithm}
\begin{itemize}
    \item \textbf{Time Complexity}: \( O(kn) \), where \( k \) is the number of iterations, typically much faster for sparse matrices.
    \item \textbf{Suitability}: Best for sparse matrices, but requires additional setup and storage for sparse matrix representations.
\end{itemize}

\section{Geometry and Visual Concept of Eigenvalues and the QR Algorithm}

\subsection{Eigenvalues Geometrically}
Eigenvalues represent the scaling factor by which a matrix stretches or compresses a vector along its eigenvectors. For a given matrix \( A \), an eigenvalue \( \lambda \) satisfies the equation:

\[
A\mathbf{v} = \lambda\mathbf{v}
\]

where \( \mathbf{v} \) is the eigenvector associated with \( \lambda \).

\subsection{Visual Representation of the QR Algorithm}
The QR algorithm iteratively transforms the matrix into a form where the eigenvalues can be directly read from the diagonal. 
\section{Conclusion}
In this report, we explored multiple eigenvalue calculation algorithms and implemented the QR algorithm for computing the eigenvalues of a matrix. The QR algorithm was chosen due to its efficiency and ability to compute all eigenvalues of both symmetric and non-symmetric matrices. A time complexity analysis showed that it has a computational cost of \( O(n^4) \), which is acceptable for medium-sized matrices. Further optimizations could include using the Lanczos method for large sparse matrices. The report also compared different algorithms, highlighting the trade-offs between time complexity, accuracy, and suitability for different types of matrices.

\end{document}

